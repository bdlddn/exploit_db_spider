#!/usr/bin/env python
# -*- coding: utf-8 -*-
#
#  webapp_spider.py
#  
#  Copyright 2017 masai <masai@MASAI-PC>
#  
#  This program is free software; you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, write to the Free Software
#  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston,
#  MA 02110-1301, USA.
#  
#  


from bs4 import BeautifulSoup
import re
import requests
import urllib.request as urllib2
import json
from elasticsearch import Elasticsearch

base_url = "https://www.exploit-db.com/webapps/"
base_index = "webapps-data"
base_file_name = "webapps_data.config"

def get_detail_name(detail):
	"""
	判断输入参数是否为指定的参数
	"""
	detail_dict = ["EDB-ID", "Author", "Published", "CVE", "Type",
	"Platform", "Exploit"]
	if detail in detail_dict:
		return detail
	else:
		return None

def store_data(data, data_id):
	"""将爬取的网页内容"""
	es = Elasticsearch()
	es.index(index = base_index, doc_type = "dic", id = data_id, 
		body = data)
	res = es.get(index=base_index, doc_type="dic", id = data_id)
	print(res['_id'])


def get_data(url, i):
	"""
	获取网页上的数据
	"""
	user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
	headers = {"User-Agent": user_agent}
	request = requests.get(url, headers = headers)
	contents = request.text
	soup = BeautifulSoup(contents, 'html.parser')
	
	data = {}
	data['title'] = soup.find('div', class_ = 'l-titlebar-content'
	).h1.get_text()
	details = soup.find_all('td')
	for detail in details:
		if detail.strong is not None:
			if get_detail_name(detail.strong.get_text()) is not None:
				detail_name  = detail.strong.get_text()
				if detail_name == "Exploit":
					detail_content = detail.a.get('href')
				else:
					detail_content = detail.get_text().strip().replace(
						detail.strong.get_text(), "").replace(":", "").strip()
				data[detail_name] = detail_content
	#~ print(data['Exploit'])
	req = urllib2.Request(data['Exploit'])
	req.add_header('User-Agent','Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36')
	fp = urllib2.urlopen(req, timeout = 3)
	fp_content = fp.read()
	data['code'] = str(fp_content)
	store_data(data, data_id)
	#~ print(data_id)

def get_urls(page_url, url_list):
	"""
	获得待爬取的网页列表
	"""
	user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.155 Safari/537.36'
	headers = {'User-Agent': user_agent}
	req = requests.get(page_url, headers = headers)
	soup = BeautifulSoup(req.text, 'html.parser')
	links = soup.find_all('td', class_ = 'description')
	for link in links:
		if link.a.text != 'title':
			url_list.add(link.a.get('href'))
	#~ print(url_list)
	next_links = soup.find('div', class_ = 'pagination').find_all('a', 
		class_='color')
	for next_link in next_links:
		if next_link.text == 'next':
			print('haha')
			print(next_link.get('href'))
			return next_link.get('href')
# 主函数入口
if __name__ == "__main__":
	remote_str = ""
	remote_config = {}
	with open(base_file_name, "r") as object_file:
		remote_str = object_file.read()
	remote_config = json.loads(remote_str)
	next_url = remote_config['next_url']
	
	while next_url is not None:
		url_list = set()
		data_id = remote_config['id']
		retry_times = 0
		try:
			next_url = get_urls(next_url, url_list)
			while len(url_list) != 0:
				try:
					data_url = url_list.pop()
					if data_url in remote_config['list']:
						data_id += 1
						continue
					data_id += 1
					get_data(data_url, data_id)
					remote_config['list'].append(data_url)
					remote_config['id'] = data_id
					with open(base_file_name, "w") as object_file:
						object_file.write(json.dumps(remote_config))
				except:
					print('get data NO.' + str(data_id) + 'wrong.')
			remote_config['next_url'] = next_url
			remote_config['list'] = []
			with open(base_file_name, "w") as object_file:
					object_file.write(json.dumps(remote_config))
		except:
			print('get page: ' + next_url + 'wrong ' + str(retry_times) + " times")
			if retry_times == 3:
				next_url = base_url + "?order_by=date_publish&order=desc&pg=" + str(data_id/50 + 2)
				retry_times = 0 
			else:
				retry_times += 1
